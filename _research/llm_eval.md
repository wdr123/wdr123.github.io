---
title: "Systematic Evaluation of Content Quality in LLMs through Token-Level Uncertainty"
layout: single-portfolio
excerpt: "<img src='/images/research/eval.png' alt=''>"
collection: research
order_number: 50
header: 
  og_image: "research/eval.png"
---

In this research, by thoroughly analyzing token-level uncertainty and introducing a suite of novel uncertainty metrics, 
we offer key insights that enable more systematic evaluation of LLM-generated content. 
A major distinction between incorrect and irrelevant content is that in the former, only few tokens are under-confident, whereas in the latter, under-confident tokens become significantly larger. 
We design novel aggregation functions that create uncertainty metrics tailored for LLM content evaluation.

